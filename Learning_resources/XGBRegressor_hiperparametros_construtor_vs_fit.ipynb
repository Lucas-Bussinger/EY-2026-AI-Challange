{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost `XGBRegressor` — Hiperparâmetros no **construtor** vs **treino** (fit)\n",
        "\n",
        "> Autor: Lucas Bussinger — Gerado pelo M365 Copilot em 2026-01-27 12:14\n",
        "> Objetivo: Demonstrar, com exemplos executáveis, **onde** cada hiperparâmetro do `XGBRegressor` é definido: no **modelo** (construtor) ou no **treino** (`fit`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sumário\n",
        "1. [Visão geral rápida](#visao-geral)\n",
        "2. [Parâmetros no **construtor** (`XGBRegressor(...)`)](#construtor)\n",
        "3. [Parâmetros no **treino** (`fit(...)`)](#treino)\n",
        "4. [Exemplo completo com validação e *early stopping*](#exemplo-completo)\n",
        "5. [Presets: GBTREE, DART e GBLINEAR](#presets)\n",
        "6. [Inspecionando todos os parâmetros do estimador](#inspecao)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visao-geral"
      },
      "source": [
        "## 1) Visão geral rápida <a id=\"visao-geral\"></a>\n",
        "- **Construtor**: define *hiperparâmetros do modelo/booster* (e.g., `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_*`, `reg_alpha`, `reg_lambda`, `tree_method`, `gamma`, `booster`, `objective`, etc.).\n",
        "- **Treino (`fit`)**: define *parâmetros de avaliação e rotina de treinamento* (e.g., `eval_set`, `eval_metric`, `early_stopping_rounds`, `verbose`, `callbacks`, `sample_weight`, `sample_weight_eval_set`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "construtor"
      },
      "source": [
        "## 2) Parâmetros no **construtor** (`XGBRegressor(...)`) <a id=\"construtor\"></a>\n",
        "São passados na criação do estimador e controlam o **booster** e a **regularização**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gbtree = XGBRegressor(\n",
        "    # --- GERAL ---\n",
        "    booster=\"gbtree\",          # [gbtree, dart] | Algoritmo base. gbtree é o padrão.\n",
        "    n_estimators=300,          # [100-1000] | Núm. de árvores. Alto: Overfitting/Lento. Baixo: Underfitting.\n",
        "    learning_rate=0.05,        # [0.01-0.3] | Passo de aprendizado. Alto: Rápido/Instável. Baixo: Robusto/Lento.\n",
        "    random_state=42,           # [Any Int] | Semente para replicabilidade.\n",
        "\n",
        "    # --- COMPLEXIDADE / ESTRUTURA (Crucial contra Overfitting) ---\n",
        "    max_depth=4,               # [3-10] | Profundidade da árvore. Alto: Overfitting (seu 8 era alto). Baixo: Underfitting.\n",
        "    min_child_weight=15.0,     # [1-20+] | Peso mín. na folha. Alto: Conservador (Geralmente melhor). Baixo: Overfitting.\n",
        "    gamma=1.0,                 # [0-10+] | Ganho mín. para split (Poda). Alto: Árvores menores. Baixo: Complexidade.\n",
        "    max_delta_step=0.0,        # [0-10] | Útil em classes desbalanceadas (raro em regressão).\n",
        "    max_leaves=0,              # [0-255] | Máx. de folhas. 0 usa max_depth. Alto: Complexo.\n",
        "    grow_policy=\"depthwise\",   # [depthwise, lossguide] | Como a árvore cresce.\n",
        "    max_bin=256,               # [256] | Discretização de variáveis. Alto: Mais precisão/Lento.\n",
        "\n",
        "    # --- AMOSTRAGEM (Aleatoriedade para Generalizar) ---\n",
        "    subsample=0.6,             # [0.5-1.0] | % de linhas por árvore. Alto: Overfitting. Baixo: Robusto.\n",
        "    colsample_bytree=0.4,      # [0.3-0.8] | % de colunas por árvore. Alto: Overfitting. Baixo: Generalização (Força outras bandas).\n",
        "    colsample_bylevel=1.0,     # [0.5-1.0] | % de colunas por nível.\n",
        "    colsample_bynode=1.0,      # [0.5-1.0] | % de colunas por nó.\n",
        "\n",
        "    # --- REGULARIZAÇÃO (Penalidade para o modelo) ---\n",
        "    reg_lambda=20.0,           # [1-100] | L2 (Ridge). Alto: Modelo simples/estável. Baixo: Complexidade/Overfitting.\n",
        "    reg_alpha=10.0,            # [0-100] | L1 (Lasso). Alto: Seleção de variáveis (ignora ruído). Baixo: Usa tudo.\n",
        "\n",
        "    # --- MÉTODO DE CONSTRUÇÃO ---\n",
        "    tree_method=\"hist\",        # [auto, exact, approx, hist] | 'hist' é o mais rápido para datasets médios/grandes.\n",
        "    sampling_method=\"uniform\", # [uniform, gradient_based] | Método de sorteio de amostras.\n",
        "\n",
        "    # --- DIVERSOS ---\n",
        "    base_score=0.5,            # Ponto de partida das predições (geralmente a média do alvo).\n",
        "    verbosity=1,               # [0,1,2,3] | Nível de detalhe dos logs de treino.\n",
        "    importance_type=\"gain\",    # [gain, weight, cover] | 'gain' é o melhor para entender o sinal real das features.\n",
        "\n",
        "    # --- OBJETIVO ---\n",
        "    objective=\"reg:squarederror\", # Erro quadrático médio para regressão.\n",
        "    enable_categorical=False      # Se True, trata colunas 'category' automaticamente.\n",
        ")\n",
        "\n",
        "# DICA: Para o seu Challenge, os valores que alterei acima (max_depth menor, \n",
        "# reg_alpha/lambda maiores e colsample menor) são focados em fechar aquele \n",
        "# \"gap\" na curva de aprendizado e melhorar sua nota na submissão."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Booster DART (parâmetros adicionais)\n",
        "Além dos parâmetros de árvores, o DART adiciona *dropout* de árvores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dart = XGBRegressor(\n",
        "    # --- GERAL (DART é mais lento, pode precisar de menos LR ou mais Estimators) ---\n",
        "    booster=\"dart\",            # Ativa o modo Dropout. Excelente contra Overfitting.\n",
        "    n_estimators=400,          # [100-1000] | Núm. de árvores. DART converge mais devagar que gbtree.\n",
        "    learning_rate=0.05,        # [0.01-0.2] | Passo. No DART, valores menores são mais estáveis.\n",
        "    \n",
        "    # --- ESTRUTURA DA ÁRVORE (Mesmos do gbtree) ---\n",
        "    max_depth=4,               # [3-6] | Profundidade. Alto: Overfitting. Baixo: Generalização. (Reduzi de 6 para 4).\n",
        "    min_child_weight=10.0,     # [1-20+] | Mín. de dados por folha. Alto: Bloqueia decoreba. Baixo: Overfitting.\n",
        "    gamma=0.5,                 # [0-5] | Penalidade para novos splits. Alto: Conservador.\n",
        "    subsample=0.6,             # [0.5-0.9] | % de linhas por árvore. Baixo: Mais aleatoriedade/Robusto.\n",
        "    colsample_bytree=0.5,      # [0.3-0.7] | % de colunas por árvore. Baixo: Força o uso de bandas fracas.\n",
        "\n",
        "    # --- ESPECÍFICOS DO DART (A mágica do Dropout) ---\n",
        "    rate_drop=0.15,            # [0.0-0.3] | Fração de árvores a serem \"dropadas\" por rodada.\n",
        "                               # Alto: Regularização forte, impede especialização. Baixo: Vira gbtree comum.\n",
        "    \n",
        "    skip_drop=0.5,             # [0.0-0.9] | Probabilidade de pular o dropout em uma iteração.\n",
        "                               # Alto: Mais chance de agir como gbtree normal. Baixo: Dropout agressivo.\n",
        "    \n",
        "    one_drop=False,            # [True, False] | Se True, pelo menos uma árvore será dropada sempre.\n",
        "    \n",
        "    max_drop=50,               # [0-n] | Máximo de árvores que podem ser dropadas por rodada. 0 = sem limite.\n",
        "    \n",
        "    sample_type=\"uniform\",     # [uniform, weighted] | Como escolher as árvores p/ dropar. \n",
        "                               # 'uniform': todas iguais. 'weighted': dropa conforme o peso.\n",
        "    \n",
        "    normalize_type=\"tree\",     # [tree, forest] | Como reescalar as árvores após o dropout.\n",
        "                               # 'tree': Nova árvore tem peso 1/k. 'forest': Mais complexo (estilo floresta).\n",
        "\n",
        "    # --- REGULARIZAÇÃO EXTRA ---\n",
        "    reg_lambda=15.0,           # [1-100] | L2. Alto: Reduz oscilações nas predições.\n",
        "    reg_alpha=10.0,            # [0-100] | L1. Alto: Ignora colunas que são apenas ruído.\n",
        "\n",
        "    # --- DIVERSOS ---\n",
        "    tree_method=\"hist\",        # Método rápido de construção.\n",
        "    importance_type=\"gain\",    # Mede a importância baseada no ganho real de informação.\n",
        "    objective=\"reg:squarederror\"\n",
        ")\n",
        "\n",
        "# NOTA PARA O CHALLENGE:\n",
        "# O seu código original tinha max_depth=6 e subsample/colsample=0.9. \n",
        "# Isso estava dando 90% de liberdade para o modelo decorar o mapa.\n",
        "# Com as reduções que fiz acima, o DART será forçado a criar padrões \n",
        "# que funcionam nos dados de teste que ele nunca viu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Booster GBLINEAR (modelo linear)\n",
        "Ignora hiperparâmetros de árvore e usa principalmente regularização.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "blinear = XGBRegressor(\n",
        "    booster=\"gblinear\",\n",
        "    n_estimators=200,      # iterações do otimizador\n",
        "    learning_rate=0.1,     # também usado no otimizador linear\n",
        "\n",
        "    # Regularização\n",
        "    reg_lambda=1.0,        # L2\n",
        "    reg_alpha=0.0,         # L1\n",
        "\n",
        "    # Algumas versões/compilações expõem:\n",
        "    # lambda_bias=0.0,     # L2 no viés (se disponível)\n",
        "    # feature_selector=\"cyclic\",  # \"cyclic\" | \"thrifty\" | \"greedy\" (contexto interno)\n",
        "    # top_k=0              # int >= 0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "treino"
      },
      "source": [
        "## 3) Parâmetros no **treino** (`fit(...)`) <a id=\"treino\"></a>\n",
        "Usados para **avaliação**, **validação**, *early stopping* e **pesos de amostra**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "\n",
        "# Dados sintéticos para exemplo\n",
        "X, y = make_regression(n_samples=5000, n_features=30, noise=15.0, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = gbtree\n",
        "\n",
        "sample_weight = np.ones(len(X_train), dtype=float)\n",
        "sample_weight_val = np.ones(len(X_val), dtype=float)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    # Avaliação/validação\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "    eval_metric=[\"rmse\", \"mae\"],  # métrica de AVALIAÇÃO\n",
        "\n",
        "    # Early stopping (requer eval_set)\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=True,\n",
        "\n",
        "    # Pesos\n",
        "    sample_weight=sample_weight,\n",
        "    sample_weight_eval_set=[sample_weight, sample_weight_val],\n",
        ")\n",
        "\n",
        "best_iter = getattr(model, 'best_iteration', None)\n",
        "best_score = getattr(model, 'best_score', None)\n",
        "best_iter, best_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb35ceae",
      "metadata": {},
      "source": [
        "## 3.2  parametros no treino(fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9abd1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Suponha X, y já preparados (numpy arrays ou pandas)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = gbtree  # de cima\n",
        "\n",
        "# Exemplo de pesos por amostra (opcional)\n",
        "sample_weight = np.ones(len(X_train), dtype=float)\n",
        "sample_weight_val = np.ones(len(X_val), dtype=float)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "\n",
        "    # ---------------------------\n",
        "    # Avaliação/validação\n",
        "    # ---------------------------\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],  # lista de tuplas\n",
        "    eval_metric=[                                   # string ou lista de strings\n",
        "        \"rmse\",     # comuns em regressão: \"rmse\", \"mae\", \"mape\"\n",
        "        # Para objetivos específicos:\n",
        "        # \"poisson-nloglik\", \"gamma-nloglik\", \"aft-nloglik\", etc.\n",
        "    ],\n",
        "\n",
        "    # ---------------------------\n",
        "    # Early stopping (para parar nos melhores rounds)\n",
        "    # ---------------------------\n",
        "    early_stopping_rounds=50,   # int >= 1 (requer eval_set)\n",
        "    verbose=True,               # imprime métricas a cada iteração\n",
        "\n",
        "    # ---------------------------\n",
        "    # Pesos e amostragem por instância\n",
        "    # ---------------------------\n",
        "    sample_weight=sample_weight,                 # (opcional)\n",
        "    sample_weight_eval_set=[sample_weight, sample_weight_val],  # (opcional)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Callbacks (avançado)\n",
        "    # ---------------------------\n",
        "    callbacks=None,             # pode passar lista de callbacks xgboost.callback.*\n",
        "    # xgb_model=None,          # caminho/objeto para continuar treinamento (warm start)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exemplo-completo"
      },
      "source": [
        "## 4) Exemplo completo com validação e *early stopping* <a id=\"exemplo-completo\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_regression(n_samples=3000, n_features=25, noise=10.0, random_state=0)\n",
        "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model_full = XGBRegressor(\n",
        "    booster=\"gbtree\",\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    min_child_weight=1.0,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.0,\n",
        "    gamma=0.0,\n",
        "    tree_method=\"hist\",\n",
        "    objective=\"reg:squarederror\",\n",
        "    random_state=42,\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "model_full.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
        "    eval_metric=[\"rmse\", \"mae\"],\n",
        "    early_stopping_rounds=100,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Melhor iteração:\", getattr(model_full, 'best_iteration', None))\n",
        "print(\"Melhor pontuação (última métrica):\", getattr(model_full, 'best_score', None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "presets"
      },
      "source": [
        "## 5) Presets: GBTREE, DART e GBLINEAR <a id=\"presets\"></a>\n",
        "Blocos prontos para reutilização em diferentes cenários.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_gbtree(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"gbtree\", n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        subsample=0.85, colsample_bytree=0.9, reg_lambda=1.0, reg_alpha=0.0,\n",
        "        tree_method=\"hist\", objective=\"reg:squarederror\", random_state=random_state\n",
        "    )\n",
        "\n",
        "def make_dart(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"dart\", n_estimators=800, learning_rate=0.05, max_depth=6,\n",
        "        subsample=0.9, colsample_bytree=0.9, rate_drop=0.1, skip_drop=0.0,\n",
        "        sample_type=\"uniform\", normalize_type=\"tree\", objective=\"reg:squarederror\",\n",
        "        tree_method=\"hist\", random_state=random_state\n",
        "    )\n",
        "\n",
        "def make_gblinear(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"gblinear\", n_estimators=400, learning_rate=0.1,\n",
        "        reg_lambda=1.0, reg_alpha=0.0, objective=\"reg:squarederror\",\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "gbtree_preset = make_gbtree()\n",
        "dart_preset = make_dart()\n",
        "gblinear_preset = make_gblinear()\n",
        "gbtree_preset, dart_preset, gblinear_preset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspecao"
      },
      "source": [
        "## 6) Inspecionando todos os parâmetros do estimador <a id=\"inspecao\"></a>\n",
        "Útil para descobrir tudo que o wrapper sklearn expõe na sua instalação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A saída pode variar conforme a versão do XGBoost instalada.\n",
        "XGBRegressor().get_params().keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Notas\n",
        "- `eval_metric` é parâmetro de **treino** (como avaliar), enquanto `objective` é do **modelo** (qual perda otimizar).\n",
        "- `early_stopping_rounds` só funciona quando `eval_set` é fornecido.\n",
        "- `n_estimators` define o **máximo** de boosting rounds; com *early stopping*, o número efetivo pode ser menor (`best_iteration`).\n",
        "- Métricas comuns na regressão: `rmse`, `mae`, `mape`; para contagem: `poisson-nloglik`; para quantis (quando suportado): `quantile`.\n",
        "- `tree_method=\"gpu_hist\"` requer XGBoost com suporte a GPU.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
