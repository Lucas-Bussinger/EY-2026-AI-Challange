{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost `XGBRegressor` — Hiperparâmetros no **construtor** vs **treino** (fit)\n",
        "\n",
        "> Autor: Lucas Bussinger — Gerado pelo M365 Copilot em 2026-01-27 12:14\n",
        "> Objetivo: Demonstrar, com exemplos executáveis, **onde** cada hiperparâmetro do `XGBRegressor` é definido: no **modelo** (construtor) ou no **treino** (`fit`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sumário\n",
        "1. [Visão geral rápida](#visao-geral)\n",
        "2. [Parâmetros no **construtor** (`XGBRegressor(...)`)](#construtor)\n",
        "3. [Parâmetros no **treino** (`fit(...)`)](#treino)\n",
        "4. [Exemplo completo com validação e *early stopping*](#exemplo-completo)\n",
        "5. [Presets: GBTREE, DART e GBLINEAR](#presets)\n",
        "6. [Inspecionando todos os parâmetros do estimador](#inspecao)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visao-geral"
      },
      "source": [
        "## 1) Visão geral rápida <a id=\"visao-geral\"></a>\n",
        "- **Construtor**: define *hiperparâmetros do modelo/booster* (e.g., `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_*`, `reg_alpha`, `reg_lambda`, `tree_method`, `gamma`, `booster`, `objective`, etc.).\n",
        "- **Treino (`fit`)**: define *parâmetros de avaliação e rotina de treinamento* (e.g., `eval_set`, `eval_metric`, `early_stopping_rounds`, `verbose`, `callbacks`, `sample_weight`, `sample_weight_eval_set`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "construtor"
      },
      "source": [
        "## 2) Parâmetros no **construtor** (`XGBRegressor(...)`) <a id=\"construtor\"></a>\n",
        "São passados na criação do estimador e controlam o **booster** e a **regularização**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# ------------------------------\n",
        "# EXEMPLO: booster = \"gbtree\" (mais comum)\n",
        "# ------------------------------\n",
        "gbtree = XGBRegressor(\n",
        "    # Geral\n",
        "    booster=\"gbtree\",          # 'gbtree' | 'dart' | 'gblinear'\n",
        "    n_estimators=500,              # int >= 1\n",
        "    learning_rate=0.05,            # (0, 1], típico 0.01–0.3 (alias de 'eta')\n",
        "    random_state=42,               # semente\n",
        "\n",
        "    # Complexidade/estrutura\n",
        "    max_depth=8,                   # int >= 0\n",
        "    min_child_weight=1.0,          # float >= 0\n",
        "    max_delta_step=0.0,            # float >= 0\n",
        "    max_leaves=0,                  # int >= 0 (0 desativa, útil com 'lossguide')\n",
        "    grow_policy=\"depthwise\",   # 'depthwise' | 'lossguide'\n",
        "    max_bin=256,                   # int >= 2 (para 'hist'/'gpu_hist')\n",
        "\n",
        "    # Amostragem (regularização estocástica)\n",
        "    subsample=0.8,                 # (0, 1]\n",
        "    colsample_bytree=0.8,          # (0, 1]\n",
        "    colsample_bylevel=1.0,         # (0, 1]\n",
        "    colsample_bynode=1.0,          # (0, 1]\n",
        "\n",
        "    # Regularização\n",
        "    reg_lambda=1.0,                # L2 >= 0\n",
        "    reg_alpha=0.0,                 # L1 >= 0\n",
        "\n",
        "    # Método de construção da árvore\n",
        "    tree_method=\"hist\",        # 'auto' | 'exact' | 'approx' | 'hist' | 'gpu_hist'\n",
        "\n",
        "    # Diversos\n",
        "    gamma=0.0,                     # >= 0, ganho mínimo para split\n",
        "    scale_pos_weight=1.0,          # >= 0 (mais útil em classificação)\n",
        "    sampling_method=\"uniform\", # 'uniform' | 'gradient_based' (para hist)\n",
        "    base_score=0.5,                # valor inicial de predição\n",
        "    verbosity=1,                   # 0,1,2,3\n",
        "    importance_type=\"gain\",    # 'gain' | 'weight' | 'cover' | 'total_gain' | 'total_cover'\n",
        "\n",
        "    # Objetivo (tarefa de regressão)\n",
        "    objective=\"reg:squarederror\",\n",
        "    enable_categorical=False\n",
        ")\n",
        "gbtree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Booster DART (parâmetros adicionais)\n",
        "Além dos parâmetros de árvores, o DART adiciona *dropout* de árvores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dart = XGBRegressor(\n",
        "    booster=\"dart\",\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.05,\n",
        "    # Todos os params de gbtree acima tbm se aplicam aqui (árvore por baixo)\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    # Específicos do DART\n",
        "    rate_drop=0.1,             # 0–1\n",
        "    skip_drop=0.0,             # 0–1\n",
        "    one_drop=False,            # bool\n",
        "    max_drop=50,               # int >= 0\n",
        "    sample_type=\"uniform\", # 'uniform' | 'weighted'\n",
        "    normalize_type=\"tree\"  # 'tree' | 'forest'\n",
        ")\n",
        "dart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Booster GBLINEAR (modelo linear)\n",
        "Ignora hiperparâmetros de árvore e usa principalmente regularização.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "blinear = XGBRegressor(\n",
        "    booster=\"gblinear\",\n",
        "    n_estimators=200,      # iterações do otimizador\n",
        "    learning_rate=0.1,     # também usado no otimizador linear\n",
        "\n",
        "    # Regularização\n",
        "    reg_lambda=1.0,        # L2\n",
        "    reg_alpha=0.0,         # L1\n",
        "\n",
        "    # Algumas versões/compilações expõem:\n",
        "    # lambda_bias=0.0,     # L2 no viés (se disponível)\n",
        "    # feature_selector=\"cyclic\",  # \"cyclic\" | \"thrifty\" | \"greedy\" (contexto interno)\n",
        "    # top_k=0              # int >= 0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "treino"
      },
      "source": [
        "## 3) Parâmetros no **treino** (`fit(...)`) <a id=\"treino\"></a>\n",
        "Usados para **avaliação**, **validação**, *early stopping* e **pesos de amostra**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "\n",
        "# Dados sintéticos para exemplo\n",
        "X, y = make_regression(n_samples=5000, n_features=30, noise=15.0, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = gbtree\n",
        "\n",
        "sample_weight = np.ones(len(X_train), dtype=float)\n",
        "sample_weight_val = np.ones(len(X_val), dtype=float)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    # Avaliação/validação\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "    eval_metric=[\"rmse\", \"mae\"],  # métrica de AVALIAÇÃO\n",
        "\n",
        "    # Early stopping (requer eval_set)\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=True,\n",
        "\n",
        "    # Pesos\n",
        "    sample_weight=sample_weight,\n",
        "    sample_weight_eval_set=[sample_weight, sample_weight_val],\n",
        ")\n",
        "\n",
        "best_iter = getattr(model, 'best_iteration', None)\n",
        "best_score = getattr(model, 'best_score', None)\n",
        "best_iter, best_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb35ceae",
      "metadata": {},
      "source": [
        "## 3.2  parametros no treino(fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9abd1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Suponha X, y já preparados (numpy arrays ou pandas)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = gbtree  # de cima\n",
        "\n",
        "# Exemplo de pesos por amostra (opcional)\n",
        "sample_weight = np.ones(len(X_train), dtype=float)\n",
        "sample_weight_val = np.ones(len(X_val), dtype=float)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "\n",
        "    # ---------------------------\n",
        "    # Avaliação/validação\n",
        "    # ---------------------------\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],  # lista de tuplas\n",
        "    eval_metric=[                                   # string ou lista de strings\n",
        "        \"rmse\",     # comuns em regressão: \"rmse\", \"mae\", \"mape\"\n",
        "        # Para objetivos específicos:\n",
        "        # \"poisson-nloglik\", \"gamma-nloglik\", \"aft-nloglik\", etc.\n",
        "    ],\n",
        "\n",
        "    # ---------------------------\n",
        "    # Early stopping (para parar nos melhores rounds)\n",
        "    # ---------------------------\n",
        "    early_stopping_rounds=50,   # int >= 1 (requer eval_set)\n",
        "    verbose=True,               # imprime métricas a cada iteração\n",
        "\n",
        "    # ---------------------------\n",
        "    # Pesos e amostragem por instância\n",
        "    # ---------------------------\n",
        "    sample_weight=sample_weight,                 # (opcional)\n",
        "    sample_weight_eval_set=[sample_weight, sample_weight_val],  # (opcional)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Callbacks (avançado)\n",
        "    # ---------------------------\n",
        "    callbacks=None,             # pode passar lista de callbacks xgboost.callback.*\n",
        "    # xgb_model=None,          # caminho/objeto para continuar treinamento (warm start)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exemplo-completo"
      },
      "source": [
        "## 4) Exemplo completo com validação e *early stopping* <a id=\"exemplo-completo\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_regression(n_samples=3000, n_features=25, noise=10.0, random_state=0)\n",
        "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model_full = XGBRegressor(\n",
        "    booster=\"gbtree\",\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    min_child_weight=1.0,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.0,\n",
        "    gamma=0.0,\n",
        "    tree_method=\"hist\",\n",
        "    objective=\"reg:squarederror\",\n",
        "    random_state=42,\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "model_full.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
        "    eval_metric=[\"rmse\", \"mae\"],\n",
        "    early_stopping_rounds=100,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Melhor iteração:\", getattr(model_full, 'best_iteration', None))\n",
        "print(\"Melhor pontuação (última métrica):\", getattr(model_full, 'best_score', None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "presets"
      },
      "source": [
        "## 5) Presets: GBTREE, DART e GBLINEAR <a id=\"presets\"></a>\n",
        "Blocos prontos para reutilização em diferentes cenários.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_gbtree(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"gbtree\", n_estimators=1000, learning_rate=0.03, max_depth=7,\n",
        "        subsample=0.85, colsample_bytree=0.9, reg_lambda=1.0, reg_alpha=0.0,\n",
        "        tree_method=\"hist\", objective=\"reg:squarederror\", random_state=random_state\n",
        "    )\n",
        "\n",
        "def make_dart(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"dart\", n_estimators=800, learning_rate=0.05, max_depth=6,\n",
        "        subsample=0.9, colsample_bytree=0.9, rate_drop=0.1, skip_drop=0.0,\n",
        "        sample_type=\"uniform\", normalize_type=\"tree\", objective=\"reg:squarederror\",\n",
        "        tree_method=\"hist\", random_state=random_state\n",
        "    )\n",
        "\n",
        "def make_gblinear(random_state=42):\n",
        "    return XGBRegressor(\n",
        "        booster=\"gblinear\", n_estimators=400, learning_rate=0.1,\n",
        "        reg_lambda=1.0, reg_alpha=0.0, objective=\"reg:squarederror\",\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "gbtree_preset = make_gbtree()\n",
        "dart_preset = make_dart()\n",
        "gblinear_preset = make_gblinear()\n",
        "gbtree_preset, dart_preset, gblinear_preset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inspecao"
      },
      "source": [
        "## 6) Inspecionando todos os parâmetros do estimador <a id=\"inspecao\"></a>\n",
        "Útil para descobrir tudo que o wrapper sklearn expõe na sua instalação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A saída pode variar conforme a versão do XGBoost instalada.\n",
        "XGBRegressor().get_params().keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Notas\n",
        "- `eval_metric` é parâmetro de **treino** (como avaliar), enquanto `objective` é do **modelo** (qual perda otimizar).\n",
        "- `early_stopping_rounds` só funciona quando `eval_set` é fornecido.\n",
        "- `n_estimators` define o **máximo** de boosting rounds; com *early stopping*, o número efetivo pode ser menor (`best_iteration`).\n",
        "- Métricas comuns na regressão: `rmse`, `mae`, `mape`; para contagem: `poisson-nloglik`; para quantis (quando suportado): `quantile`.\n",
        "- `tree_method=\"gpu_hist\"` requer XGBoost com suporte a GPU.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
